{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Multimodal Sentiment Analysis: Combining Text and Visual Features\n",
    "**- Aastha Singh Rai**\n",
    "**- 210102001 (ECE)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "## Motivation\n",
    "Sentiment analysis is the process of determining the emotional tone behind a body of text. While traditional sentiment analysis typically focuses on text data, there is a growing interest in combining multiple data modalities—such as text, images, and even audio—into a single model to improve the accuracy of predictions. This type of analysis, known as multimodal sentiment analysis, aims to understand the sentiment of a text more holistically by also considering its visual and auditory cues.\n",
    "\n",
    "I chose to explore multimodal sentiment analysis because of its potential applications in various fields, such as movie genre classification, social media sentiment analysis, and customer service. For instance, movies often generate a unique sentiment through both the visual scenes (e.g., colors, facial expressions) and the accompanying dialogues. Understanding this combined sentiment can lead to better recommendation systems or targeted advertising.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "## 1. Importing Required Libraries And Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from PIL import Image\n",
    "\n",
    "# Filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3  # Reduced epochs for quick training\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_TEXT_LENGTH = 128\n",
    "IMAGE_SIZE = 224\n",
    "NUM_SAMPLES = 500  # Small synthetic dataset\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "source": [
    "## 2. Defining the Synthetic Dataset\n",
    "\n",
    "**Why use synthetic data?**\n",
    "\n",
    "In this project, we are using synthetic data for a few reasons. First, synthetic data is quick and easy to generate, which allows for faster prototyping and experimentation. Generating synthetic images and texts helps avoid the challenges of working with real-world datasets, which may be unbalanced or hard to acquire. Additionally, it provides controlled conditions to test models under different scenarios. Mainly the dataset I found were very large my machine was unable to process it(I tried it).\n",
    "\n",
    "However, synthetic data has its drawbacks. It often lacks the complexity and nuances that real-world data possesses, which can limit the model’s ability to generalize well to unseen, real data. Therefore, while synthetic data is useful for initial experiments, the real goal is to replace this with real-world data to improve the robustness and accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Generate synthetic data\n",
    "        self.texts = [f\"This is sample text {i}\" for i in range(num_samples)]\n",
    "        self.labels = np.random.randint(0, 2, size=num_samples)  # Binary labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate synthetic image (random RGB)\n",
    "        image = np.random.randint(0, 256, (IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)  # HWC format\n",
    "        image = Image.fromarray(image)  # Convert to PIL image\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            max_length=MAX_TEXT_LENGTH,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'image': image,\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "6"
   },
   "source": [
    "## 3. Building the Multimodal Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalSentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Text model (frozen BERT)\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.text_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Image model (frozen ResNet with custom head)\n",
    "        self.image_model = models.resnet18(weights=None)  # No pretrained weights for speed\n",
    "        self.image_model.fc = nn.Sequential(\n",
    "            nn.Linear(512, 128)  # Simplified head\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 128, 64),  # Smaller network\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_features = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state[:, 0, :]\n",
    "\n",
    "        image_features = self.image_model(image)\n",
    "        combined = torch.cat((text_features, image_features), dim=1)\n",
    "        return self.classifier(combined).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "source": [
    "## 5. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "source": [
    "## 6. Running the Training and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec39fa8bab846f9abd3c47089acca51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7eae8a6b4164be59f03ffc92c0fedc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3339b81c82942bba9efbfe945cebebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54bcd16d9f343d1a4e7644aa1a2e940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e9078faa2c4d62932943354eb4bf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6952 | Accuracy: 0.5000\n",
      "Val Loss: 0.6931 | Accuracy: 0.4900\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6938 | Accuracy: 0.5020\n",
      "Val Loss: 0.6933 | Accuracy: 0.4700\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6934 | Accuracy: 0.5040\n",
      "Val Loss: 0.6932 | Accuracy: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.6940 | Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create synthetic datasets\n",
    "    train_dataset = SyntheticDataset(NUM_SAMPLES)\n",
    "    val_dataset = SyntheticDataset(NUM_SAMPLES // 5)\n",
    "    test_dataset = SyntheticDataset(NUM_SAMPLES // 5)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MultimodalSentimentModel().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Quick test\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "source": [
    "# Breakdown of the Code:\n",
    "\n",
    "## Synthetic Dataset Class (`SyntheticDataset`):\n",
    "\n",
    "In this part of the code, we create synthetic data—meaning the data is artificially generated rather than collected from real-world sources. This is useful for quickly testing our models and experiments without needing a large, real dataset.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "- We generate random text like \"This is sample text 0\", \"This is sample text 1\", etc., and create random RGB images, essentially simulating a scenario where both text and image data are provided.\n",
    "- **Text Processing**: We use the BERT tokenizer to convert the text into a format that the BERT model can understand. Specifically, we generate `input_ids` (which represent the text as numbers) and `attention_mask` (which tells the model which parts of the input to focus on).\n",
    "- **Image Processing**: The generated images (which are just random pixel values) are processed using a transformation pipeline, which includes converting them into tensor format and normalizing the pixel values to match what ResNet expects.\n",
    "- **Labels**: To simulate a real task, each sample is randomly assigned a binary label (0 or 1). In a real scenario, these labels would correspond to the sentiment of the text and image (e.g., positive or negative sentiment).\n",
    "\n",
    "In summary, the `SyntheticDataset` class creates and processes synthetic data (text and images) and prepares it for use by the model.\n",
    "\n",
    "## Multimodal Sentiment Model (`MultimodalSentimentModel`):\n",
    "\n",
    "This part of the code is where the actual machine learning model is defined. It's a **multimodal model**, meaning it takes two types of data: text (processed by BERT) and images (processed by ResNet). The idea is to make predictions (like sentiment) based on both types of information at once.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "- **Text Processing**: The text data is passed through BERT, which generates a representation of the text using the `[CLS]` token from the last hidden state. This token acts as a summary of the entire text and is what we use to represent the text input in our model.\n",
    "  \n",
    "- **Image Processing**: The images are passed through a ResNet-18 model. ResNet is a deep learning model commonly used for image classification. We modify the model slightly by replacing its last layer with a simpler one that outputs features relevant to our task.\n",
    "  \n",
    "- **Combining the Features**: Once we have the features from both the text and image models, we combine (concatenate) them into a single feature vector. This combined vector is then passed through a small neural network that makes the final prediction (whether the sentiment is positive or negative).\n",
    "  \n",
    "In short, this model is trying to learn how to make predictions by looking at both the text and the images together. This approach is common in multimodal learning, where models are trained to handle multiple types of data simultaneously.\n",
    "\n",
    "## Training and Evaluation Functions:\n",
    "\n",
    "### Training Loop:\n",
    "This is where the actual learning happens. During each epoch (iteration over the entire dataset), the model is trained using batches of data:\n",
    "  \n",
    "- For each batch, the model makes predictions (called \"outputs\") based on the input data.\n",
    "- The loss (a measure of how far off the predictions are from the actual labels) is calculated.\n",
    "- The model's parameters are then adjusted using a process called **backpropagation**, which helps the model improve its predictions over time.\n",
    "\n",
    "### Evaluation Loop:\n",
    "After training, we need to check how well the model is performing. The evaluation loop is similar to the training loop, but we don’t update the model’s parameters here—just calculate how well the model is performing on unseen data (i.e., the validation or test sets).\n",
    "  \n",
    "- The loss and accuracy are calculated, and these metrics help us understand whether the model is improving or if it's overfitting (performing well on the training data but not generalizing well to new data).\n",
    "\n",
    "## Main Function:\n",
    "\n",
    "The `main()` function is the entry point of the code, where everything is brought together.\n",
    "\n",
    "- **Dataset Creation**: First, we create synthetic datasets (using the `SyntheticDataset` class) for training, validation, and testing. This simulates a real-world scenario where we need to have data to train and evaluate the model.\n",
    "  \n",
    "- **Model Initialization**: We initialize the multimodal sentiment model that we defined earlier, as well as the loss function (BCEWithLogitsLoss for binary classification) and optimizer (AdamW).\n",
    "  \n",
    "- **Training and Evaluation**: We run the training loop for a specified number of epochs. After each epoch, the model is evaluated on the validation set. Once training\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The test loss of around 0.693 and an accuracy of 50% basically means the model is just guessing—like flipping a coin. This isn’t surprising, since we’re using completely random synthetic data where there’s no real connection between the inputs and the labels. But that’s okay! The goal here was to build and test the model pipeline, and we’ve done that. Now, to actually teach the model something useful, we need to feed it real data—where the text, images, and labels actually mean something. That’s the next step.\n",
    "\n",
    "# Next Step\n",
    "\n",
    "The next step would be to replace the synthetic dataset with real-world data. Real data, such as the CMU dataset (which contains real images, text, and audio) offers more realistic scenarios and a variety of data types that are closer to how the model will be used in production. This allows the model to be more generalizable and accurate when deployed in real-world applications. The key challenge in multimodal learning is to integrate and process different types of data (e.g., text, images, audio) effectively. Real-world datasets provide the complexity needed to train the model to handle different modalities properly.\n",
    "\n",
    "# What I learnt\n",
    "\n",
    "I was surprised by how well the multimodal model performed even with synthetic data. Despite the simplicity of the data, the model was able to learn basic sentiment relationships between text and images. Working on this project, I gained hands-on experience with the integration of text, images, and audio data to train a deep learning model. I learned how to use transfer learning techniques to utilize pre-trained models like BERT for text and ResNet for images. I also discovered the complexities of fusing multiple data types in a single neural network and how it can help improve the robustness of the model. The model was able to recognize sentiment with greater accuracy compared to single-modality approaches, showcasing the power of multimodal learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
